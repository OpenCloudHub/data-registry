question,expected_answer,key_concepts,category
"What is OpenCloudHub?","OpenCloudHub is a comprehensive MLOps platform built on Kubernetes that demonstrates the complete machine learning lifecycle for both predictive and generative AI workloads, using open-source self-hostable components.","[""MLOps"", ""Kubernetes"", ""machine learning""]",overview
"What are the main components of OpenCloudHub?","The platform integrates ArgoCD for GitOps, MLflow for experiment tracking, DVC for data versioning, Argo Workflows for pipeline orchestration, Ray for distributed computing, and the LGTM stack for observability.","[""ArgoCD"", ""MLflow"", ""DVC"", ""Ray"", ""LGTM""]",overview
"How does OpenCloudHub handle GitOps deployments?","ArgoCD with ApplicationSets manages GitOps deployments, automatically synchronizing Kubernetes resources from Git repositories and enabling declarative infrastructure management.","[""ArgoCD"", ""ApplicationSets"", ""GitOps""]",infrastructure
"What is the observability stack used?","The LGTM stack provides observability: Prometheus for metrics collection, Grafana for dashboards, Loki for log aggregation, and Tempo for distributed tracing.","[""Prometheus"", ""Grafana"", ""Loki"", ""Tempo"", ""LGTM""]",observability
"How is experiment tracking handled?","MLflow tracks experiments including hyperparameters, metrics, and artifacts. It also serves as the model registry with lifecycle stages and alias management for deployment.","[""MLflow"", ""experiment tracking"", ""model registry""]",mlops
"How does data versioning work?","DVC versions datasets with Git integration, storing actual data in MinIO object storage while tracking metadata in Git. This enables reproducible data pipelines with explicit lineage to training runs.","[""DVC"", ""data versioning"", ""MinIO""]",mlops
"What object storage solution is used?","MinIO provides S3-compatible object storage for ML artifacts, model files, DVC data, and training checkpoints.","[""MinIO"", ""S3"", ""object storage""]",infrastructure
"What database does the platform use?","CloudNativePG operates PostgreSQL databases in Kubernetes, used for MLflow metadata, application data, and pgvector for embedding storage.","[""CloudNativePG"", ""PostgreSQL""]",infrastructure
"How does pipeline orchestration work?","Argo Workflows orchestrates ML pipelines as directed acyclic graphs with dependency management, retry logic, and integration with MLflow for tracking.","[""Argo Workflows"", ""pipeline"", ""DAG""]",mlops
"What ML frameworks are supported?","The platform demonstrates framework agnosticism with scikit-learn, PyTorch Lightning, HuggingFace Transformers, and custom PyTorch for Qwen-VL fine-tuning.","[""scikit-learn"", ""PyTorch Lightning"", ""Transformers""]",mlops
"How does model serving work?","Ray Serve deploys models as inference endpoints with autoscaling, health checks, and support for both traditional ML models and LLMs via vLLM backend.","[""Ray Serve"", ""inference"", ""autoscaling""]",mlops
"How does OpenCloudHub serve LLMs?","Ray Serve with vLLM backend provides LLM serving featuring continuous batching, streaming token generation, and OpenAI-compatible API endpoints.","[""Ray Serve"", ""vLLM"", ""LLM"", ""streaming""]",genai
"What is fractional GPU allocation?","KubeRay enables fractional GPU allocation allowing multiple models to share a single GPU. For example, Qwen serving uses 0.49 GPU allocation.","[""KubeRay"", ""fractional GPU"", ""GPU sharing""]",genai
"How does distributed training work?","Ray Train with PyTorch DDP strategy enables distributed data parallel training across multiple workers, with checkpoints stored in MinIO for fault tolerance.","[""Ray Train"", ""DDP"", ""distributed training""]",mlops
"What hyperparameter optimization approach is used?","Ray Tune with ASHA scheduler performs hyperparameter optimization, early-stopping underperforming trials to efficiently focus compute on promising configurations.","[""Ray Tune"", ""ASHA"", ""hyperparameter optimization""]",mlops
"What vector database is used for RAG?","The pgvector extension for PostgreSQL stores embeddings, enabling similarity search for retrieval-augmented generation pipelines.","[""pgvector"", ""PostgreSQL"", ""vector database""]",genai
"How are embeddings generated?","Sentence-transformers models generate embeddings in data pipelines, which are versioned with DVC and stored in pgvector for retrieval.","[""sentence-transformers"", ""embeddings"", ""DVC""]",genai
"How does the RAG pipeline work?","The RAG pipeline retrieves relevant context from pgvector based on query embeddings, then generates responses using Qwen LLM with the retrieved context.","[""RAG"", ""pgvector"", ""retrieval"", ""generation""]",genai
"How does prompt versioning work?","MLflow Prompt Registry versions prompt templates as first-class artifacts, enabling evaluation tracking and controlled deployment of prompt changes.","[""MLflow"", ""prompt versioning"", ""prompt registry""]",genai
"How is prompt evaluation performed?","LLM-as-judge evaluation assesses prompt quality using configurable criteria, with results tracked in MLflow for comparison across prompt versions.","[""LLM-as-judge"", ""evaluation"", ""MLflow""]",genai
"What is the model promotion workflow?","Models progress through stages in MLflow registry using aliases. The champion alias marks production models, with promotion triggered after validation.","[""MLflow"", ""model registry"", ""champion"", ""promotion""]",mlops
"How does environment promotion work?","Namespaces separate ci, staging, and production environments. Argo Workflows orchestrate promotion with validation gates between stages.","[""namespaces"", ""staging"", ""production"", ""promotion""]",mlops
"How does CI/CD integration work?","GitHub Actions with Actions Runner Controller executes CI pipelines in-cluster, triggering Argo Workflows for training and deployment automation.","[""GitHub Actions"", ""ARC"", ""CI/CD""]",infrastructure
"What Kubernetes distribution is used?","Minikube provides local Kubernetes development with GPU passthrough for the NVIDIA RTX 4070 Ti SUPER, enabling realistic GPU workload testing.","[""Minikube"", ""GPU"", ""local development""]",infrastructure
"What workloads demonstrate the platform?","Five workloads validate the platform: wine classification with sklearn, Fashion MNIST with PyTorch Lightning, emotion classification with DistilBERT, Qwen-VL fine-tuning, and RAG pipeline.","[""wine"", ""Fashion MNIST"", ""emotion"", ""Qwen-VL"", ""RAG""]",workloads
"How does the wine classifier workload work?","The wine classifier uses scikit-learn with MLflow tracking, demonstrating the basic ML lifecycle including training, registry, and Ray Serve deployment.","[""wine"", ""scikit-learn"", ""MLflow""]",workloads
"What does the Fashion MNIST workload demonstrate?","Fashion MNIST validates distributed training using PyTorch Lightning with Ray Train DDP strategy across multiple workers with GPU acceleration.","[""Fashion MNIST"", ""PyTorch Lightning"", ""distributed""]",workloads
"How does the emotion classifier work?","The emotion classifier fine-tunes DistilBERT using Ray Tune with ASHA scheduler for hyperparameter optimization, tracking all trials in MLflow.","[""emotion"", ""DistilBERT"", ""Ray Tune"", ""ASHA""]",workloads
"What is the Qwen-VL workload?","Qwen-VL fine-tuning demonstrates vision-language model training with GPU scheduling, producing a model for multimodal educational content analysis.","[""Qwen-VL"", ""vision-language"", ""fine-tuning""]",workloads
"How does distributed tracing work?","OpenTelemetry instrumentation with Tempo backend enables end-to-end request tracing across frontend, backend, and ML services with correlation IDs.","[""OpenTelemetry"", ""Tempo"", ""tracing"", ""correlation""]",observability